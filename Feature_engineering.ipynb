{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08y6rOnBiifC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###1. What is a parameter?\n",
        "\n",
        "A parameter is a value or a variable that is used to define or influence the behavior of a system, function, or process. Parameters are commonly used in different fields such as mathematics, programming, engineering, and statistics.\n",
        "\n",
        "###2. What is correlation?What does negative correlation mean?\n",
        "\n",
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It indicates how changes in one variable are associated with changes in another. Correlation is commonly used in fields like statistics, finance, science, and social sciences to understand relationships between data points.\n",
        "\n",
        "Negative Correlation:\n",
        "\n",
        "When one variable increases, the other variable decreases.\n",
        "\n",
        "Example: As exercise time increases, body weight tends to decrease.\n",
        "\n",
        "###3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed. It involves developing algorithms that allow systems to improve their performance on a task by recognizing patterns and relationships in data.\n",
        "\n",
        "Main Components in Machine Learning:\n",
        "\n",
        "Data:\n",
        "\n",
        "The raw information used to train the model.\n",
        "Must be collected, cleaned, and preprocessed.\n",
        "\n",
        "Features:\n",
        "\n",
        "Specific characteristics or properties of the data used for learning.\n",
        "Feature engineering and selection are critical for accuracy.\n",
        "\n",
        "Model:\n",
        "\n",
        "The mathematical representation of the learning process.\n",
        "Used to identify patterns and make predictions.\n",
        "\n",
        "Algorithms:\n",
        "\n",
        "The set of rules the model uses to learn from data.\n",
        "Examples: Linear Regression, Neural Networks, Decision Trees.\n",
        "\n",
        "Training Process:\n",
        "\n",
        "The process of teaching the model using historical data.\n",
        "Adjusts parameters to minimize prediction errors.\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "Measures model performance using metrics like accuracy, precision, and recall.\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "Configurable settings that influence how the model learns (e.g., learning rate).\n",
        "\n",
        "Prediction/Inference:\n",
        "\n",
        "Using the trained model to make predictions on new, unseen data.\n",
        "\n",
        "Deployment:\n",
        "\n",
        "Integrating the trained model into real-world applications for practical use.\n",
        "\n",
        "Feedback Loop:\n",
        "\n",
        "Continuous improvement by retraining the model with new data.\n",
        "\n",
        "###4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Loss value is a crucial metric in machine learning that quantifies how well a model's predictions match the actual target values. It helps in assessing the model's performance by providing a numerical measure of the error.\n",
        "\n",
        "Key Points on How Loss Value Helps:\n",
        "Measures Prediction Error\n",
        "\n",
        "Guides Model Training\n",
        "\n",
        "Overfitting vs. Underfitting Detection\n",
        "\n",
        "High training loss\n",
        "\n",
        "Comparison of Different Models\n",
        "\n",
        "Convergence Monitoring\n",
        "\n",
        "Choosing the Right Model Complexity\n",
        "\n",
        "\n",
        "###5. What are continuous and categorical variables?\n",
        "\n",
        "1. Continuous Variables\n",
        "Definition:\n",
        "Continuous variables can take an infinite number of values within a given range. These values are typically numerical and measurable, meaning they can have decimal points and fractions.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Can be measured (e.g., weight, temperature, time).\n",
        "Values can be infinitely precise (e.g., 25.3°C, 10.567 kg).\n",
        "Arithmetic operations like addition and subtraction make sense.\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 175.5 cm)\n",
        "Temperature (e.g., 36.8°C)\n",
        "Salary (e.g., $50,000.75)\n",
        "Distance (e.g., 12.3 km)\n",
        "Common Continuous Data Types in Machine Learning:\n",
        "\n",
        "Interval Data: No true zero point (e.g., temperature in Celsius).\n",
        "Ratio Data: Has a meaningful zero point (e.g., weight, age).\n",
        "\n",
        "2. Categorical Variables\n",
        "\n",
        "Definition:\n",
        "\n",
        "Categorical variables represent data that can be divided into distinct groups or categories. These values are qualitative rather than quantitative.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Represent different categories or labels.\n",
        "Cannot perform arithmetic operations on them.\n",
        "Categories may have no intrinsic order (nominal) or may follow a defined order (ordinal).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender (Male, Female, Non-binary)\n",
        "Color (Red, Blue, Green)\n",
        "Education Level (High School, Bachelor's, Master's)\n",
        "Type of Vehicle (Car, Truck, Bike)\n",
        "Types of Categorical Variables:\n",
        "\n",
        "Nominal Variables:\n",
        "\n",
        "Categories have no inherent order.\n",
        "Example: Marital Status (Single, Married, Divorced).\n",
        "\n",
        "Ordinal Variables:\n",
        "\n",
        "Categories have a meaningful order but no fixed spacing.\n",
        "Example: Customer Satisfaction (Low, Medium, High).\n",
        "Handling in Machine Learning:\n",
        "\n",
        "Categorical variables often need to be converted into numerical form using techniques like:\n",
        "One-hot encoding (for nominal data).\n",
        "Label encoding (for ordinal data).\n",
        "\n",
        "###6. How do we handle categorical variables in Machine Learning?What are the common techniques?\n",
        "\n",
        "Categorical variables need to be converted into numerical form before they can be used in machine learning models, as most algorithms work with numerical data. Handling categorical data properly is crucial for improving model accuracy and performance.\n",
        "\n",
        "Common Techniques to Handle Categorical Variables:\n",
        "1. One-Hot Encoding\n",
        "Definition:\n",
        "\n",
        "Converts each unique category into a new binary (0/1) column.\n",
        "Suitable for nominal (unordered) categorical variables.\n",
        "\n",
        "2. Label Encoding\n",
        "Definition:\n",
        "\n",
        "Assigns each category a unique integer value.\n",
        "Suitable for ordinal (ordered) categorical variables.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Definition:\n",
        "\n",
        "Similar to label encoding but with predefined order of categories.\n",
        "Used for ordinal variables with an inherent ranking.\n",
        "\n",
        "4. Target Encoding (Mean Encoding)\n",
        "Definition:\n",
        "\n",
        "Replaces categories with the mean (or another statistic) of the target variable for each category.\n",
        "Suitable for categorical variables in regression problems.\n",
        "\n",
        "5. Frequency Encoding\n",
        "Definition:\n",
        "\n",
        "Replaces categories with their frequency in the dataset.\n",
        "Helps in reducing dimensionality while retaining useful information.\n",
        "\n",
        "6. Binary Encoding\n",
        "Definition:\n",
        "\n",
        "Converts categories to binary representation and stores in multiple columns.\n",
        "A balance between one-hot encoding and label encoding.\n",
        "\n",
        "7. Hash Encoding (Hashing Trick)\n",
        "Definition:\n",
        "\n",
        "Maps categorical values to a fixed number of hash buckets.\n",
        "Useful for high-cardinality categorical features.\n",
        "\n",
        "\n",
        "###7. What do you mean by training and testing a dataset?\n",
        "\n",
        "In machine learning, training and testing datasets are used to build and evaluate models effectively. They help in ensuring that the model generalizes well to unseen data and does not overfit the training data.\n",
        "\n",
        "1. Training Dataset\n",
        "Definition:\n",
        "\n",
        "The training dataset is the portion of the data used to teach the machine learning model.\n",
        "It contains input features along with their corresponding output labels (for supervised learning).\n",
        "The model learns patterns, relationships, and structures from this data by adjusting its internal parameters.\n",
        "Purpose:\n",
        "\n",
        "To train the model by optimizing its weights/parameters using algorithms such as Gradient Descent.\n",
        "The goal is to minimize the loss/error by learning from the data patterns.\n",
        "\n",
        "2. Testing Dataset\n",
        "Definition:\n",
        "\n",
        "The testing dataset is the portion of data used to evaluate the trained model's performance.\n",
        "It contains unseen data that was not used during training to check how well the model generalizes to new inputs.\n",
        "Purpose:\n",
        "\n",
        "To assess how accurately the model can predict outcomes on new, unseen data.\n",
        "To detect overfitting (when the model memorizes the training data but fails to generalize).\n",
        "\n",
        "\n",
        "###8. What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in the Scikit-Learn library that provides various utilities to preprocess data before feeding it into machine learning models. Preprocessing is an essential step to ensure that the data is formatted correctly and optimized for learning algorithms to achieve better performance.\n",
        "\n",
        "Why is Preprocessing Important?\n",
        "\n",
        "Raw data often contains inconsistencies such as:\n",
        "\n",
        "Different scales for numerical values\n",
        "Missing or categorical data\n",
        "Outliers and noise\n",
        "Features with varying importance\n",
        "The sklearn.preprocessing module helps transform raw data into a format that improves the efficiency and accuracy of machine learning models.\n",
        "\n",
        "\n",
        "###9. What is a Test set?\n",
        "\n",
        "A test set is a subset of the dataset that is used to evaluate the performance of a trained machine learning model. It consists of unseen data that was not used during the model training process. The test set helps in assessing how well the model generalizes to new, real-world data.\n",
        "\n",
        "Purpose of a Test Set\n",
        "The test set is used to:\n",
        "\n",
        "Evaluate Model Performance: Measure accuracy, precision, recall, F1-score, etc.\n",
        "Detect Overfitting: Ensure the model doesn't just memorize training data but generalizes well.\n",
        "Compare Different Models: Helps in selecting the best-performing model.\n",
        "Estimate Real-World Performance: Provides insights into how the model will perform on unseen data.\n",
        "\n",
        "###10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "\n",
        "Splitting data into training and testing sets is crucial to build and evaluate machine learning models effectively. In Python, the most common method for splitting data is by using the train_test_split function from Scikit-Learn.\n"
      ],
      "metadata": {
        "id": "TGLkgEjNiji1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q0XXUaglDkq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample feature and target data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([0, 1, 0, 1, 0])  # Labels\n",
        "\n",
        "# Splitting the dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Testing Features:\\n\", X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNlKSizWDX4q",
        "outputId": "6f912079-c229-4ed5-c0d6-04a92b1b0ca4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features:\n",
            " [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "Testing Features:\n",
            " [[3 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J59i7eE3DUnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a crucial step before training a machine learning model because it helps us understand the dataset, identify potential issues, and make informed decisions about preprocessing and feature selection. EDA allows us to detect patterns, spot anomalies, and gain insights that can improve the model's performance and reliability.\n",
        "\n",
        "###12.  What is correlation?\n",
        "\n",
        "Correlation is a statistical measure that describes the relationship between two or more variables. It indicates how one variable changes in response to changes in another. The correlation coefficient quantifies the strength and direction of this relationship.\n",
        "\n",
        "###13.  What does negative correlation mean?\n",
        "\n",
        "A negative correlation means that as one variable increases, the other variable decreases, and vice versa. In other words, the two variables move in opposite directions.\n",
        "\n",
        "###14.  How can you find correlation between variables in Python?\n",
        "\n",
        "n Python, you can calculate the correlation between variables using various libraries such as Pandas, NumPy, and SciPy. These libraries provide easy-to-use functions to compute correlation coefficients for different types of relationships.\n",
        "\n",
        "###15.  What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation, also known as causality, refers to a relationship between two variables where a change in one variable directly causes a change in the other. In other words, causation implies a cause-and-effect relationship.\n",
        "\n",
        "Key Characteristics of Causation:\n",
        "\n",
        "One variable directly influences another.\n",
        "\n",
        "There is a clear mechanism explaining how the cause leads to the effect.\n",
        "\n",
        "Experiments or strong observational studies are usually required to establish causation.\n",
        "\n",
        "###16.  What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "An optimizer is an algorithm used in machine learning and deep learning to minimize the loss function by adjusting the model’s parameters (such as weights and biases). The goal is to find the optimal set of parameters that allow the model to make accurate predictions by reducing the error between predicted and actual values.\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "Concept:\n",
        "\n",
        "It updates the parameters in the direction of the negative gradient to minimize the loss.\n",
        "Types of Gradient Descent:\n",
        "\n",
        "Batch Gradient Descent: Computes gradients over the entire dataset.\n",
        "Stochastic Gradient Descent (SGD): Updates parameters using one data point at a time.\n",
        "Mini-batch Gradient Descent: Updates parameters using a small batch of data.\n",
        "\n",
        "2. Momentum Optimization\n",
        "Concept:\n",
        "\n",
        "Improves gradient descent by adding momentum to accelerate learning.\n",
        "\n",
        "3. Adagrad (Adaptive Gradient Algorithm)\n",
        "Concept:\n",
        "\n",
        "Adapts learning rate for each parameter by scaling based on past gradients.\n",
        "\n",
        "4. RMSprop (Root Mean Square Propagation)\n",
        "Concept:\n",
        "\n",
        "Maintains a moving average of squared gradients to normalize the learning rate.\n",
        "\n",
        "5. Adam (Adaptive Moment Estimation)\n",
        "Concept:\n",
        "\n",
        "Combines momentum and adaptive learning rate approaches.\n",
        "Maintains estimates of both the first moment (mean) and the second moment (variance) of gradients.\n",
        "\n",
        "6. Adamax (Variant of Adam using Infinity Norm)\n",
        "Concept:\n",
        "\n",
        "Variant of Adam optimizer suitable for high-dimensional spaces.\n",
        "\n",
        "7. Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n",
        "Concept:\n",
        "\n",
        "Combines Adam with Nesterov momentum to improve performance.\n"
      ],
      "metadata": {
        "id": "lQ6-4VZwDnUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "\n",
        "#Momentum Optimization\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "#Adagrad (Adaptive Gradient Algorithm) Concept\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "\n",
        "#RMSprop (Root Mean Square Propagation)\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "\n",
        "#Adam (Adaptive Moment Estimation) Concept\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "#Adamax (Variant of Adam using Infinity Norm) Concept\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "\n",
        "optimizer = Adamax(learning_rate=0.002)\n",
        "\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "\n",
        "optimizer = Nadam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "8HXzXNMtFapd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrLsE2GFG4JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. What is sklearn.linear_model ?\n",
        "\n",
        "sklearn.linear_model is a module in Scikit-learn, a popular machine learning library in Python, that provides various linear models for regression and classification tasks. These models assume a linear relationship between the input features and the target variable.\n",
        "\n",
        "###18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "In machine learning, model.fit() is a method used to train a model. This means that it fits the model to the training data, allowing it to learn the relationship between the input features (X) and the target variable (y). The fit() function is responsible for:\n",
        "\n",
        "Training the Model: It uses the training data to adjust the model's parameters (weights and biases).\n",
        "Learning the Patterns: It attempts to find the optimal solution to minimize the loss function (error) by adjusting the model parameters.\n",
        "Fitting to Data: Once the model is trained, it will be able to make predictions based on what it has learned from the data.\n",
        "\n",
        "Arguments to Pass to model.fit()\n",
        "The main arguments that must be given are:\n",
        "\n",
        "X (features/input data):\n",
        "\n",
        "This is the data that the model uses to learn the relationships.\n",
        "It is typically a 2D array or matrix (for example, n_samples x n_features in shape).\n",
        "Each row represents a sample (observation), and each column represents a feature (variable).\n",
        "For example, in a dataset with features like height, weight, and age, X would contain these values for each sample.\n",
        "y (target/output data):\n",
        "\n",
        "This is the target variable (the values you want to predict).\n",
        "It could be either a continuous variable (regression task) or categorical labels (classification task).\n",
        "y is typically a 1D array (for regression) or 2D for multi-class classification.\n",
        "Optional arguments include:\n",
        "\n",
        "epochs (for deep learning models):\n",
        "\n",
        "Defines the number of iterations over the entire training dataset.\n",
        "batch_size (for batch learning models):\n",
        "\n",
        "Defines the number of samples per gradient update (usually used in neural networks).\n",
        "validation_data (optional):\n",
        "\n",
        "Used to evaluate the model's performance after each epoch (in deep learning models).\n",
        "You can pass a tuple (X_val, y_val) to monitor validation loss and accuracy.\n",
        "\n",
        "\n",
        "###19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "The model.predict() method is used to make predictions with a trained model. After a model has been trained using model.fit(), model.predict() applies the learned patterns (parameters/weights) to new, unseen input data and produces predictions.\n",
        "\n",
        "For regression models: The prediction is a continuous numerical value.\n",
        "For classification models: The prediction is a class label or probabilities for each class (depending on the model).\n",
        "\n",
        "Arguments to Pass to model.predict()\n",
        "The main argument that must be given is:\n",
        "\n",
        "X (input data):\n",
        "This is the data for which you want to make predictions.\n",
        "It must have the same shape as the training data (i.e., the same number of features).\n",
        "X can be a 1D or 2D array depending on the model, and it should contain the feature values for the instances you want to predict.\n",
        "Optional Arguments:\n",
        "\n",
        "Some models allow additional arguments, but X is the primary required argument for making predictions.\n",
        "\n",
        "###19. What are continuous and categorical variables?\n",
        "\n",
        "Continuous Variables\n",
        "Continuous variables are variables that can take on an infinite number of values within a given range. These values are often measurements or quantities that can be subdivided into smaller increments. They are typically numeric and can take any value within a specific interval (including fractions or decimals).\n",
        "\n",
        "Categorical Variables\n",
        "Categorical variables (also called qualitative variables) are variables that represent categories or labels. They describe characteristics or qualities that can be placed into specific groups or categories, but these categories don't have a meaningful order or ranking (for nominal categories). In some cases, categorical variables may have a natural order (for ordinal categories).\n",
        "\n",
        "###21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Feature scaling is the process of standardizing or normalizing the range of independent variables or features in a dataset. It involves transforming the features into a similar scale, so that they have comparable magnitudes, preventing any one feature from dominating the learning process due to its scale.\n",
        "\n",
        "In machine learning, many algorithms rely on the relative scale of the features, and if features have different scales, the model may perform poorly or converge slowly.\n",
        "\n",
        "Why is Feature Scaling Important?\n",
        "Improves Convergence Speed:\n",
        "Many optimization algorithms (like gradient descent) converge faster when features are scaled similarly. If one feature has a much larger range than others, the model might get stuck or take a longer time to find the optimal solution.\n",
        "\n",
        "Prevents Dominance of Certain Features:\n",
        "Features with large values can dominate the model, making it biased toward those features. For instance, in a dataset where one feature represents \"age\" (e.g., 10 to 100) and another represents \"income\" (e.g., 1000 to 100,000), income would dominate in distance-based models like k-NN or gradient-based algorithms like logistic regression.\n",
        "\n",
        "Improves Accuracy:\n",
        "Algorithms like k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), and Principal Component Analysis (PCA) rely on distance calculations, which are sensitive to the scale of the data. Without scaling, features with large numerical values can disproportionately influence these algorithms, leading to poor performance.\n",
        "\n",
        "Ensures Consistent Contribution of Features:\n",
        "When all features are scaled, each one contributes equally to the learning process. Otherwise, features with larger ranges might overshadow smaller-range features.\n",
        "\n",
        "###22. How do we perform scaling in Python?\n",
        "\n",
        "\n",
        "In Python, scaling can be performed using libraries like Scikit-learn. Scikit-learn provides various preprocessing methods for scaling data, such as Min-Max Scaling, Standardization, Robust Scaling, and more. Here's how you can perform scaling using Scikit-learn:\n",
        "\n",
        "1. Min-Max Scaling (Normalization)\n",
        "Min-Max scaling scales the data to a specific range, typically [0, 1].\n",
        "\n",
        "2. Standardization (Z-Score Normalization)\n",
        "Standardization scales the data to have a mean of 0 and a standard deviation of 1\n",
        "\n",
        "3. Robust Scaling\n",
        "Robust scaling uses the median and interquartile range (IQR) to scale the data, making it more robust to outliers.\n",
        "\n",
        "4. MaxAbs Scaling\n",
        "MaxAbs scaling scales the data by its maximum absolute value, making the data fall within the range [-1, 1] without shifting or squashing it.\n",
        "\n",
        "5. Handling Categorical Data (One-Hot Encoding)\n",
        "In addition to scaling, we often need to handle categorical data. One common method is One-Hot Encoding.\n",
        "\n",
        "###23. What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-learn, a popular Python library for machine learning. This module provides a set of tools and techniques to preprocess data, preparing it for use in machine learning models. Preprocessing is an essential step in the machine learning pipeline, as it helps improve model performance by transforming raw data into a format that is suitable for algorithms.\n",
        "\n",
        "###24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "o split data for model fitting (training and testing) in Python, the train_test_split function from Scikit-learn is typically used. This function divides the dataset into two sets: a training set and a testing set. The training set is used to train the machine learning model, and the testing set is used to evaluate the model's performance on unseen data.\n",
        "\n",
        "Steps to Split Data:\n",
        "Import the necessary libraries.\n",
        "Load your dataset (either from a CSV file, DataFrame, or other sources).\n",
        "Use train_test_split to split the dataset into training and testing sets.\n",
        "\n",
        "Syntax of train_test_split:\n",
        "\n",
        "[from sklearn.model_selection import train_test_split]\n",
        "\n",
        "###25. Explain data encoding?\n",
        "\n",
        "Data encoding is the process of converting categorical variables (those that contain labels or categories) into a format that can be understood and processed by machine learning algorithms. Since most machine learning models expect numerical input, encoding is used to transform categorical data (like strings or labels) into numerical values or binary format, allowing these algorithms to handle and learn from the data.\n",
        "\n",
        "Data encoding can be done in several ways, depending on the type of data and the problem at hand. Two of the most common encoding techniques are Label Encoding and One-Hot Encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "t3ATQcyXG59O"
      }
    }
  ]
}